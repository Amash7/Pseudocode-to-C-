{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TkVCVm8X3tj3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sentencepiece as spm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import math\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Enable CUDA debugging for better error tracking\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "train_path = \"/content/spoc-train-train.tsv\"\n",
        "val_path = \"/content/spoc-train-eval.tsv\"\n",
        "test_path = \"/content/spoc-train-test.tsv\"\n",
        "\n",
        "train_df = pd.read_csv(train_path, sep=\"\\t\")\n",
        "val_df = pd.read_csv(val_path, sep=\"\\t\")\n",
        "test_df = pd.read_csv(test_path, sep=\"\\t\")\n",
        "\n",
        "# Drop rows where pseudocode ('text') is missing\n",
        "train_df = train_df.dropna(subset=[\"text\"])\n",
        "val_df = val_df.dropna(subset=[\"text\"])\n",
        "test_df = test_df.dropna(subset=[\"text\"])\n",
        "\n",
        "# Combine lines of pseudocode and code per problem and submission\n",
        "def group_data(df):\n",
        "    return df.groupby([\"probid\", \"subid\"]).agg({\n",
        "        \"text\": lambda x: \"\\n\".join(x),\n",
        "        \"code\": lambda x: \"\\n\".join(x)\n",
        "    }).reset_index()\n",
        "\n",
        "train_df = group_data(train_df)\n",
        "val_df = group_data(val_df)\n",
        "test_df = group_data(test_df)\n",
        "\n",
        "print(\"Preprocessing complete. Data loaded and structured.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQEgN6md36uL",
        "outputId": "ced41dce-23bf-4b0b-c24f-432628363172"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing complete. Data loaded and structured.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenizing\n",
        "# Prepare training data for tokenizer\n",
        "all_text = list(train_df[\"text\"]) + list(train_df[\"code\"])\n",
        "\n",
        "# Train SentencePiece tokenizer directly from in-memory data\n",
        "spm.SentencePieceTrainer.train(\n",
        "    sentence_iterator=iter(all_text),\n",
        "    model_prefix=\"/content/spoc_tokenizer\",\n",
        "    vocab_size=24000,\n",
        "    character_coverage=1.0,\n",
        "    model_type=\"bpe\"\n",
        ")\n",
        "\n",
        "# Load trained tokenizer\n",
        "sp = spm.SentencePieceProcessor(model_file=\"/content/spoc_tokenizer.model\")\n",
        "\n",
        "# Special tokens\n",
        "sos_token = sp.piece_to_id(\"<s>\")\n",
        "eos_token = sp.piece_to_id(\"</s>\")\n",
        "pad_token = 23999  # Set padding token to 0 or another valid index\n",
        "\n",
        "# Tokenize datasets\n",
        "def tokenize_data(df, sp):\n",
        "    df[\"text_tokenized\"] = df[\"text\"].apply(lambda x: sp.encode(x, out_type=int))\n",
        "    df[\"code_tokenized\"] = df[\"code\"].apply(lambda x: sp.encode(x, out_type=int))\n",
        "    return df\n",
        "\n",
        "train_df = tokenize_data(train_df, sp)\n",
        "val_df = tokenize_data(val_df, sp)\n",
        "test_df = tokenize_data(test_df, sp)\n"
      ],
      "metadata": {
        "id": "CCMJmORH51Yu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace invalid tokens (-1) with the padding token\n",
        "def replace_invalid_tokens(sequences, pad_token):\n",
        "    return [[pad_token if token == -1 else token for token in seq] for seq in sequences]\n",
        "\n",
        "train_df[\"text_tokenized\"] = replace_invalid_tokens(train_df[\"text_tokenized\"], pad_token)\n",
        "train_df[\"code_tokenized\"] = replace_invalid_tokens(train_df[\"code_tokenized\"], pad_token)\n",
        "val_df[\"text_tokenized\"] = replace_invalid_tokens(val_df[\"text_tokenized\"], pad_token)\n",
        "val_df[\"code_tokenized\"] = replace_invalid_tokens(val_df[\"code_tokenized\"], pad_token)\n",
        "test_df[\"text_tokenized\"] = replace_invalid_tokens(test_df[\"text_tokenized\"], pad_token)\n",
        "test_df[\"code_tokenized\"] = replace_invalid_tokens(test_df[\"code_tokenized\"], pad_token)\n",
        "\n",
        "# Pad sequences to max length\n",
        "def pad_sequences(sequences, max_length, pad_value):\n",
        "    return [seq[:max_length] + [pad_value] * max(0, max_length - len(seq)) for seq in sequences]\n",
        "\n",
        "max_length = 256  # Define max sequence length\n",
        "train_df[\"text_tokenized\"] = pad_sequences(train_df[\"text_tokenized\"], max_length, pad_token)\n",
        "train_df[\"code_tokenized\"] = pad_sequences(train_df[\"code_tokenized\"], max_length, pad_token)\n",
        "val_df[\"text_tokenized\"] = pad_sequences(val_df[\"text_tokenized\"], max_length, pad_token)\n",
        "val_df[\"code_tokenized\"] = pad_sequences(val_df[\"code_tokenized\"], max_length, pad_token)\n",
        "test_df[\"text_tokenized\"] = pad_sequences(test_df[\"text_tokenized\"], max_length, pad_token)\n",
        "test_df[\"code_tokenized\"] = pad_sequences(test_df[\"code_tokenized\"], max_length, pad_token)\n",
        "\n",
        "# PyTorch Dataset class\n",
        "class PseudocodeDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.inputs = torch.tensor(df[\"text_tokenized\"].tolist(), dtype=torch.long)\n",
        "        self.targets = torch.tensor(df[\"code_tokenized\"].tolist(), dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.inputs[idx], self.targets[idx]\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 4  # Reduce batch size\n",
        "train_dataset = PseudocodeDataset(train_df)\n",
        "val_dataset = PseudocodeDataset(val_df)\n",
        "test_dataset = PseudocodeDataset(test_df)\n",
        "\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Define vocab_size\n",
        "vocab_size = 24000\n",
        "# Function to check for invalid indices and NaN/Inf values\n",
        "def check_target_range(tgt):\n",
        "    print(\"Target min index:\", tgt.min().item())\n",
        "    print(\"Target max index:\", tgt.max().item())\n",
        "    print(\"Vocab size:\", vocab_size)\n",
        "    print(\"Padding token index:\", pad_token)\n",
        "\n",
        "def check_for_nan_inf(tensor):\n",
        "    if torch.isnan(tensor).any():\n",
        "        print(\"NaN values found!\")\n",
        "    if torch.isinf(tensor).any():\n",
        "        print(\"Inf values found!\")\n",
        "\n",
        "# Check the range of token indices in your dataset\n",
        "for src, tgt in train_loader:\n",
        "    check_target_range(tgt)\n",
        "    check_for_nan_inf(src)\n",
        "    check_for_nan_inf(tgt)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TM1EKrwa54ET",
        "outputId": "129b61f2-ed82-447a-c7e7-d91d458ae621"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target min index: 3\n",
            "Target max index: 23999\n",
            "Vocab size: 24000\n",
            "Padding token index: 23999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=512):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]"
      ],
      "metadata": {
        "id": "7cOF0h2J561H"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer Model\n",
        "class TransformerSeq2Seq(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=512, nhead=8, num_layers=8, dim_feedforward=4096, dropout=0.1):\n",
        "        super(TransformerSeq2Seq, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model)  # Add positional encoding\n",
        "        self.dropout = nn.Dropout(dropout)  # Add dropout\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            num_encoder_layers=num_layers,\n",
        "            num_decoder_layers=num_layers,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src = self.embedding(src).permute(1, 0, 2)\n",
        "        src = self.pos_encoder(src)  # Apply positional encoding\n",
        "        src = self.dropout(src)  # Apply dropout\n",
        "        tgt = self.embedding(tgt).permute(1, 0, 2)\n",
        "        tgt = self.pos_encoder(tgt)  # Apply positional encoding\n",
        "        tgt = self.dropout(tgt)  # Apply dropout\n",
        "        output = self.transformer(src, tgt)\n",
        "        return self.fc_out(output.permute(1, 0, 2))\n",
        "\n",
        "# Model Initialization\n",
        "vocab_size = 24000\n",
        "model = TransformerSeq2Seq(vocab_size).to(device)\n",
        "\n",
        "# Loss and Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.98), eps=1e-9)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_token)\n",
        "# Training Loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for src, tgt in train_loader:\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        try:\n",
        "            output = model(src, tgt[:, :-1])\n",
        "            loss = criterion(output.reshape(-1, vocab_size), tgt[:, 1:].reshape(-1))\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        except RuntimeError as e:\n",
        "            print(\"Error during training:\", e)\n",
        "            print(\"Skipping batch...\")\n",
        "            continue\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}\")\n",
        "\n",
        "    # Validation Loop\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in val_loader:\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "            output = model(src, tgt[:, :-1])\n",
        "            loss = criterion(output.reshape(-1, vocab_size), tgt[:, 1:].reshape(-1))\n",
        "            val_loss += loss.item()\n",
        "    print(f\"Validation Loss: {val_loss / len(val_loader)}\")\n",
        "\n",
        "# Save trained model\n",
        "model_path = \"/content/transformer_seq2seq.pth\"\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"Model saved at {model_path}\")\n",
        "\n",
        "print(\"Model training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bx1l0c3b59kR",
        "outputId": "3ff0bd78-59b1-478f-e6d4-6d0b307fc8c9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 3.0628425938482504\n",
            "Validation Loss: 2.095234539456472\n",
            "Epoch 2, Loss: 2.112791581256451\n",
            "Validation Loss: 1.8497987571590675\n",
            "Epoch 3, Loss: 1.8977895653098924\n",
            "Validation Loss: 1.7664432168720725\n",
            "Epoch 4, Loss: 1.7670134185793331\n",
            "Validation Loss: 1.7060394243684833\n",
            "Epoch 5, Loss: 1.6729491220258492\n",
            "Validation Loss: 1.6683010728416328\n",
            "Epoch 6, Loss: 1.596191001798774\n",
            "Validation Loss: 1.6618177847353046\n",
            "Epoch 7, Loss: 1.5361280166217637\n",
            "Validation Loss: 1.6475590344912516\n",
            "Epoch 8, Loss: 1.488094523538739\n",
            "Validation Loss: 1.6356678132168547\n",
            "Epoch 9, Loss: 1.443554959803468\n",
            "Validation Loss: 1.6355549616371086\n",
            "Epoch 10, Loss: 1.4027465101664964\n",
            "Validation Loss: 1.634308675924937\n",
            "Model saved at /content/transformer_seq2seq.pth\n",
            "Model training complete.\n"
          ]
        }
      ]
    }
  ]
}